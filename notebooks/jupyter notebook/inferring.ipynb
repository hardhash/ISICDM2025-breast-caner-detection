{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a52971-6776-4969-b52c-931336764cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ultralytics import YOLO\n",
    "from timm import create_model\n",
    "from torchvision import transforms\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Config():\n",
    "    YOLO_WEIGHT_DIR = r\"E:\\ISICDM2025\\yolo_weight\"\n",
    "    CLS_WEIGHT_DIR = r\"E:\\ISICDM2025\\cls_weight\"\n",
    "    TEST_IMG_DIR = r\"E:\\ISICDM2025\\ISICDM2025_images_for_test\"\n",
    "    IMG_SIZE = 512\n",
    "    NUM_CLASSES = 7\n",
    "    YOLO_CONF_THRESH = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf7bbb3-9134-4295-901f-2ecc5b9cd506",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading YOLO model (size {Config.IMG_SIZE})...\")\n",
    "weight_path = os.path.join(Config.YOLO_WEIGHT_DIR, f\"{Config.IMG_SIZE}yolov8.pt\")\n",
    "yolo_model = YOLO(weight_path)\n",
    "yolo_model.to(DEVICE)\n",
    "\n",
    "print(f\"Loading EfficientNet classifiers (size {Config.IMG_SIZE})...\")\n",
    "cls_models = []\n",
    "for fold in range(1, 6):\n",
    "    WEIGHT_PATH = os.path.join(Config.CLS_WEIGHT_DIR, f\"{Config.IMG_SIZE}_efficientnet_b0_fold_{fold}.pth\")\n",
    "    model = create_model('tf_efficientnet_b0.ns_jft_in1k', pretrained=False, in_chans=1)\n",
    "    checkpoint = torch.load(WEIGHT_PATH, map_location=DEVICE)\n",
    "    num_classes = Config.NUM_CLASSEAS\n",
    "    model.classifier = nn.Linear(model.classifier.in_features, num_classes)\n",
    "    pretrained_dict = checkpoint['model_state_dict']\n",
    "    model_dict = model.state_dict()\n",
    "    filtered_dict = {\n",
    "        k: v for k, v in pretrained_dict.items()\n",
    "        if k in model_dict and 'classifier' not in k\n",
    "    }\n",
    "    model_dict.update(filtered_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "    model = model.to(DEVICE)\n",
    "    model.eval()\n",
    "    cls_models.append(model)\n",
    "\n",
    "def get_transform(img_size):\n",
    "    return transforms.Compose([\n",
    "        transforms.ToPILImage(mode='L'),\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.ToTensor(),  # 输出 [1, H, W]\n",
    "        transforms.Normalize(mean=[0.98755298], std=[0.026713483])\n",
    "    ])\n",
    "\n",
    "results = []\n",
    "test_images = sorted([f for f in os.listdir(Config.TEST_IMG_DIR) if f.endswith('.png')])\n",
    "\n",
    "for img_name in tqdm(test_images):\n",
    "    img_path = os.path.join(Config.TEST_IMG_DIR, img_name)\n",
    "    orig_img = cv2.imread(img_path)  # 读为三通道 BGR（YOLO 要求）\n",
    "    if orig_img is None:\n",
    "        print(f\"Warning: cannot read {img_path}\")\n",
    "        continue\n",
    "    h, w = orig_img.shape[:2]\n",
    "\n",
    "    # Step 1: YOLO 推理（仅 512）\n",
    "    results_yolo = yolo_model(\n",
    "        orig_img,\n",
    "        imgsz=Config.IMG_SIZE,\n",
    "        conf=Config.YOLO_CONF_THRESH,\n",
    "        verbose=False\n",
    "    )\n",
    "    pred = results_yolo[0].boxes\n",
    "\n",
    "    if pred is None or len(pred) == 0:\n",
    "        continue\n",
    "\n",
    "    boxes_xyxy = pred.xyxy.cpu()  # [N, 4]\n",
    "    confs = pred.conf.cpu()       # [N]\n",
    "\n",
    "    # Step 2: 对每个框进行分类预测\n",
    "    for box, det_conf in zip(boxes_xyxy, confs):\n",
    "        x1, y1, x2, y2 = box.int().tolist()\n",
    "        x1, y1 = max(0, x1), max(0, y1)\n",
    "        x2, y2 = min(w, x2), min(h, y2)\n",
    "        if x2 <= x1 or y2 <= y1:\n",
    "            continue\n",
    "\n",
    "        crop = orig_img[y1:y2, x1:x2]  # 三通道 BGR\n",
    "        if crop.size == 0:\n",
    "            continue\n",
    "\n",
    "        crop_gray = cv2.cvtColor(crop, cv2.COLOR_BGR2GRAY)  # [H, W]\n",
    "\n",
    "        total_logits = torch.zeros(Config.NUM_CLASSEAS, device=DEVICE)\n",
    "        transform = get_transform(Config.IMG_SIZE)\n",
    "        input_tensor = transform(crop_gray).unsqueeze(0).to(DEVICE)  # [1, 1, H, W]\n",
    "        fold_logits = []\n",
    "        for model in cls_models:\n",
    "            with torch.no_grad():\n",
    "                logits = model(input_tensor)\n",
    "                fold_logits.append(logits)\n",
    "        avg_logits = torch.mean(torch.cat(fold_logits, dim=0), dim=0)\n",
    "        total_logits += avg_logits\n",
    "\n",
    "        probs = F.softmax(total_logits, dim=0)\n",
    "        pred_class = torch.argmax(probs).item()\n",
    "        confidence = probs[pred_class].item()\n",
    "\n",
    "        results.append({\n",
    "            'image_name': img_name,\n",
    "            'xmin': x1,\n",
    "            'ymin': y1,\n",
    "            'xmax': x2,\n",
    "            'ymax': y2,\n",
    "            'predicted_class': pred_class,\n",
    "            'confidence': confidence\n",
    "        })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
