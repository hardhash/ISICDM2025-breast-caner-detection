{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21484c69-0a11-401a-a354-bafd2c434709",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\ultra\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from timm import create_model\n",
    "from sklearn.model_selection import KFold\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96afb445-22e2-494c-b113-18ab8ceb2c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    DATA_ROOT = r'E:\\ISICDM2025\\competition_image'\n",
    "    CSV_PATH = os.path.join(DATA_ROOT, 'isicdm2025.csv')\n",
    "    WEIGHTS_DIR = r'E:\\ISICDM2025'\n",
    "    IMG_SIZES = [384, 512, 768]  # 可选: 384, 512, 768\n",
    "    BATCH_SIZE = 16\n",
    "    NUM_EPOCHS = 200\n",
    "    LEARNING_RATE = 5e-5\n",
    "    PATIENCE = 7\n",
    "    RANDOM_SEED = 42\n",
    "    \n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d20da2-acb2-473c-bb80-7574d3836fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ISICDMDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.dataframe = dataframe.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        # 构造完整路径: E:\\ISICDM2025\\competition_image\\train\\ISICDM2025_000399.png\n",
    "        img_path = os.path.join(Config.DATA_ROOT, row['split'], row['crop_filename'])\n",
    "        image = Image.open(img_path).convert(\"L\")  # 单通道灰度图\n",
    "        label = row['category_id']\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=Config.PATIENCE, verbose=False, delta=0, path='best_model.pth'):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = float('inf')\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "\n",
    "    def __call__(self, val_loss, model, optimizer, epoch):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, optimizer, epoch)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, optimizer, epoch)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model, optimizer, epoch):\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}). Saving model ...')\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "        }, self.path)\n",
    "        self.val_loss_min = val_loss\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for images, labels in loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    acc = 100. * correct / total\n",
    "    return total_loss / len(loader), acc\n",
    "\n",
    "def validate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    acc = 100. * correct / total\n",
    "    return total_loss / len(loader), acc\n",
    "\n",
    "        \n",
    "for IMG_SIZE in Config.IMG_SIZES:\n",
    "    WEIGHT_PATH = os.path.join(Config.WEIGHTS_DIR, f'{IMG_SIZE}_efficientnet_b0_expand.pth')\n",
    "    SAVE_PATH = f'finetuned_all_{IMG_SIZE}_efficientnet_b0.pth'\n",
    "    temp_df = pd.read_csv(Config.CSV_PATH)\n",
    "    train_temp_df = temp_df[temp_df['split'] == 'train']  # 仅用原始train计算统计量，避免数据泄露\n",
    "    \n",
    "    temp_transform = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "    \n",
    "    temp_dataset = ISICDMDataset(train_temp_df, temp_transform)\n",
    "    temp_loader = DataLoader(temp_dataset, batch_size=64, pin_memory=True)\n",
    "    \n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    nb_samples = 0.\n",
    "    \n",
    "    for data, _ in temp_loader:\n",
    "        batch_samples = data.size(0)\n",
    "        data = data.view(batch_samples, data.size(1), -1)  # [B, 1, H*W]\n",
    "        mean += data.mean(2).sum(0)\n",
    "        std += data.std(2).sum(0)\n",
    "        nb_samples += batch_samples\n",
    "    \n",
    "    MEAN = [mean.item() / nb_samples]\n",
    "    STD = [std.item() / nb_samples]\n",
    "    \n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomRotation(15),\n",
    "        transforms.ColorJitter(brightness=0.3, contrast=0.3),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=MEAN, std=STD)\n",
    "    ])\n",
    "    \n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=MEAN, std=STD)\n",
    "    ])\n",
    "\n",
    "    df = pd.read_csv(Config.CSV_PATH)\n",
    "    all_data_df = pd.concat([\n",
    "        df[df['split'] == 'train'],\n",
    "        df[df['split'] == 'val'],\n",
    "        df[df['split'] == 'test']\n",
    "    ], ignore_index=True)\n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=Config.RANDOM_SEED)\n",
    "    all_metrics = []\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(all_data_df), 1):\n",
    "        print(f'====== Starting Fold {fold} ======')\n",
    "        \n",
    "        # Split data for current fold\n",
    "        train_df = all_data_df.iloc[train_idx].reset_index(drop=True)\n",
    "        val_df = all_data_df.iloc[val_idx].reset_index(drop=True)\n",
    "    \n",
    "        # Create datasets and dataloaders\n",
    "        train_dataset = ISICDMDataset(train_df, train_transform)\n",
    "        val_dataset = ISICDMDataset(val_df, val_transform)\n",
    "    \n",
    "        train_loader = DataLoader(train_dataset, batch_size=Config.BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=Config.BATCH_SIZE, shuffle=False, pin_memory=True)\n",
    "    \n",
    "        # Initialize model\n",
    "        model = create_model('tf_efficientnet_b0.ns_jft_in1k', pretrained=False, in_chans=1)\n",
    "        checkpoint = torch.load(WEIGHT_PATH, map_location=DEVICE, weights_only=False)\n",
    "    \n",
    "        num_classes = 7\n",
    "        model.classifier = nn.Linear(model.classifier.in_features, num_classes)\n",
    "    \n",
    "        # Load pretrained weights (excluding classifier)\n",
    "        pretrained_dict = checkpoint['model_state_dict']\n",
    "        model_dict = model.state_dict()\n",
    "        filtered_dict = {\n",
    "            k: v for k, v in pretrained_dict.items()\n",
    "            if k in model_dict and 'classifier' not in k  # 跳过分类头\n",
    "        }\n",
    "        model_dict.update(filtered_dict)\n",
    "        model.load_state_dict(model_dict)\n",
    "        model = model.to(DEVICE)\n",
    "    \n",
    "        # Loss, optimizer, scheduler\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=Config.LEARNING_RATE)\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max = Config.NUM_EPOCHS * len(train_loader),\n",
    "            eta_min=1e-6\n",
    "        )\n",
    "    \n",
    "        # Early stopping for this fold\n",
    "        early_stopping = EarlyStopping(patience=PATIENCE, verbose=True, path=f'{SAVE_PATH}_fold_{fold}.pth')\n",
    "    \n",
    "        # Training loop\n",
    "        for epoch in range(Config.NUM_EPOCHS):\n",
    "            print(f\"\\n---------- Epoch {epoch+1}/{Config.NUM_EPOCHS} ----------\")\n",
    "    \n",
    "            # Training step\n",
    "            train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, DEVICE)\n",
    "            \n",
    "            # Validation step\n",
    "            val_loss, val_acc = validate(model, val_loader, criterion, DEVICE)\n",
    "    \n",
    "            # Print metrics\n",
    "            print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "            print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "            # Early stopping check\n",
    "            early_stopping(val_loss, model, optimizer, epoch)\n",
    "            if early_stopping.early_stop:\n",
    "                print(f\"Early stopping triggered for Fold {fold} at epoch {epoch+1}\")\n",
    "                break\n",
    "    \n",
    "            # Scheduler step\n",
    "            scheduler.step()\n",
    "            # current_lr = scheduler.get_last_lr()[0] # Not used, removed\n",
    "    \n",
    "        # Record best metric for this fold\n",
    "        all_metrics.append({'fold': fold, 'best_val_loss': early_stopping.val_loss_min})\n",
    "        print(f'====== Completed Fold {fold} ======\\n')\n",
    "    print(f\"\\n{IMG_SIZE} - 微调训练完成！各折最佳模型已保存至:\")\n",
    "    for i in range(5):\n",
    "        print(f\"  - {SAVE_PATH}_fold_{i+1}.pth\")\n",
    "    print(\"\\n各折最佳验证损失:\")\n",
    "    for metric in all_metrics:\n",
    "        print(f\"  - Fold {metric['fold']}: {metric['best_val_loss']:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431596c5-269b-47da-b84d-0fd05fd9995a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca1adca-24c9-46fe-a1b1-25af1ecd3e76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
